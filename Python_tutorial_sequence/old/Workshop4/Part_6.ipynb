{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fitting Models to Data: Frequentist vs. Bayesian Approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://homepages.wmich.edu/~wlacefie/images/evobayes.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Download and install the needed packages "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You'll need 'emcee' and 'corner' for the tutorial below. Your computer may already have these programs installed. You can check if it does by seeing if the cell below runs without error. The lines of interest are:\n",
    "* import emcee \n",
    "* import corner\n",
    "\n",
    "If the cell below gives you an error, then:\n",
    "* Click <a href=\"http://dan.iel.fm/emcee/current/user/install/\">here</a> for information on emcee. There are install instructions on that page.  I recommend to use the command line 'pip' to install. \n",
    "* Click <a href=\"https://github.com/dfm/corner.py\">here</a> for information on corner.  Again, I recommend using pip to install corner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#After you've downloaded the above packages, import the needed libraries\n",
    "#you'll need numpy, matplotlib, pyplot, emcee, corner (if necessary, see previous workshops for how to import these)\n",
    "---\n",
    "\n",
    "#we also want these\n",
    "from scipy.optimize import leastsq\n",
    "import multiprocessing as multi\n",
    "nthreads = multi.cpu_count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Exploring Random Number Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First let's talk about drawing values randomly in general.\n",
    "\n",
    "Computers don't know how to draw truly random numbers. Instead they generate some large sequence of numbers that appear random, but are really deterministic. Usually you have to define a \"seed\" value to define where to start the sequence.\n",
    "\n",
    "In python you don't actually have to define the \"seed\" value explicitly, but you can in order to keep your \"random\" draw the same each time you run the code.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Defining the \"seed\" for numpy's random function\n",
    "numpy.random.seed(seed = 1234567)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's draw randomly from a Gaussian (aka 'Normal') distribution. <br>\n",
    "Note: A Gaussian distribution looks like: <img src=\"http://zoonek2.free.fr/UNIX/48_R/g587.png\" width=300>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Numpy has a built-in function to draw random numbers from a Gaussian\n",
    "#define the Gaussian using the mean, standard deviation (stdev), \n",
    "#  and choose some number of values (Nvals) that you want to draw\n",
    "mean =  ---\n",
    "stdev = ---\n",
    "Nvals = ---\n",
    "rg = numpy.random.normal(mean, stdev, size = Nvals)\n",
    "\n",
    "#print the mean, min and max of this list of random mumbers\n",
    "print 'mean,min,max of the random sample', ---, ---, ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#plot a histogram of these values\n",
    "n, bins, patches = pyplot.hist(rg, 50, color = 'b', alpha = 0.6, range = [mean - 5.*stdev, mean + 5.*stdev])\n",
    "\n",
    "#overplot a dashed line that shows the mean value\n",
    "pyplot.plot(---)\n",
    "\n",
    "pyplot.xlabel(\"value\")\n",
    "pyplot.ylabel(\"N\")\n",
    "pyplot.draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#How many values are within 1,2,3 sigma of the mean for this numpy.random.normal function?\n",
    "#Hint: you can use numpy.where to find all the values that are within some range of the mean\n",
    "for fac in [1,2,3]:\n",
    "    ---\n",
    "    print \"percent inside of %f times sigma = %f\" % (fac, ---)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question: Do the numbers for sigma match up to the <a href=\"http://en.wikipedia.org/wiki/68%E2%80%9395%E2%80%9399.7_rule\">right answer for a gaussian distribution</a>?\n",
    "\n",
    "If not exactly, why not? What could you change to get closer to the right answer? Make this change above, and see if you can come closer to the correct values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#What if you didn't have this easy numpy.random.normal function?\n",
    "#You could instead sample a Gaussian using this \"Box-Muller\" procedure \n",
    "#with two random numbers between 0 and 1. \n",
    "#In python you can draw numbers from a uniform distribution with numpy.random.random()\n",
    "#This method is very useful, e.g., in C or fortran (and many othe codes)\n",
    "rg_bm = []\n",
    "for i in range(Nvals):\n",
    "#draw your first random number\n",
    "    x1 = ---\n",
    "#draw your second random number    \n",
    "    x2 = ---\n",
    "#just in case\n",
    "    while (x1 == 0): x1 = numpy.random.random()\n",
    "    while (x2 == 0): x2 = numpy.random.random()\n",
    "#this draws from a Guassian distribution\n",
    "    rg_bm.append(stdev * (-2.*numpy.log(x1))**0.5 * numpy.cos(2.*numpy.pi*x2) + mean)\n",
    "    \n",
    "rg_bm = numpy.array(rg_bm) #just to keep them in the same format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#How many values are within 1,2,3 sigma for the \"Box-Muller\" procedure?\n",
    "for fac in [1,2,3]:\n",
    "    ---\n",
    "    print \"percent inside of %f times sigma = %f\" % (fac, ---)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#plot a histogram of these values\n",
    "\n",
    "#first the numpy.random.normal results\n",
    "---\n",
    "#now the Box-Muller results\n",
    "---\n",
    "\n",
    "#overplot a dashed line that shows the mean value\n",
    "---\n",
    "\n",
    "#overplot dotted lines that show the 1,2,3 sigma values\n",
    "---\n",
    "    \n",
    "pyplot.xlabel(\"value\")\n",
    "pyplot.ylabel(\"N\")\n",
    "pyplot.draw()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Generating Observed Orbit Data for a Spectroscopic Binary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we know a little more about random number generation, let's apply it to an astrophysical context. \n",
    "\n",
    "Let's simulate observed data for a spectroscopic binary with a time series of radial velocites. We want to fit an orbital solution to these observations.\n",
    "\n",
    "The image illustrates binary stars' motion over time, their spectra, and radial velocity. <img src=\"http://plato.acadiau.ca/courses/phys/1523/jan18/specbiny.gif\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate 'observed' heliocentric radial velocity of a spectroscopic binary. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#The following code is based on helio_rv.pro, from IDL's astronomy library \n",
    "#note: omega is given in degrees\n",
    "\n",
    "#JD = Julian Date\n",
    "#for your birthday in JD, check out http://aa.usno.navy.mil/data/docs/JulianDate.php\n",
    "\n",
    "def helio_rv(JD, params, lim = 1.e-8, nlim = 1000.):\n",
    "    T, P, V0, K, e, omega = params\n",
    "    \n",
    "    #safety check\n",
    "    rv = [1e10 for i in JD]\n",
    "    if (e >= 0. and e < 1. and P > 0. and K > 0. and omega >= 0. and omega <= 360.):\n",
    "\n",
    "        # Calculate the approximate eccentric anomaly, E1, via the mean anomaly, M.\n",
    "        # (from Heintz DW, \"Double stars\", Reidel, 1978)\n",
    "        M = numpy.array([2. * numpy.pi * ( (x - T) / P % 1.) for x in JD])\n",
    "        E1 = numpy.array([x + e*numpy.sin(x)  + ( (e**2.) * numpy.sin(2.*x) / 2. ) for x in M])\n",
    "\n",
    "        # Refine this estimate using formulae given by Reidel.\n",
    "        test = 2*lim\n",
    "        i = 0\n",
    "        nlim = 1000.\n",
    "        while (test > lim):\n",
    "            i += 1\n",
    "            E0 = E1\n",
    "            M0 = E0 - e * numpy.sin(E0)\n",
    "            E1 = E0 + (M - M0) / (1. - e*numpy.cos(E0))\n",
    "            use = numpy.where(E1 != 0)[0]\n",
    "            if (len(use) > 0):\n",
    "                test = max(abs( (E1[use] - E0[use])/E1[use]) )\n",
    "            else:\n",
    "                test = lim\n",
    "            if (i > nlim):\n",
    "                test = lim\n",
    "                    \n",
    "        # Calculate nu\n",
    "        nu = 2. * numpy.arctan( ((1. + e)/(1. - e))**0.5 * numpy.tan(E1/2.) )\n",
    "\n",
    "        # Calculate radial velocities\n",
    "        rv = (K* (numpy.cos(nu + omega*numpy.pi/180.) + (e*numpy.cos(omega*numpy.pi/180.)) ) ) + V0\n",
    "    \n",
    "    return rv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Set the number of observations (feel free to change this)\n",
    "Nobs = 20\n",
    "\n",
    "#Set the observation dates\n",
    "drange = 50.\n",
    "JD = [drange*x  for x in numpy.random.random(Nobs)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Here we set the true orbital parameters and determine the true radial velocities for the primary star.\n",
    "#Note: below you'll add 'noise' to account for measurement precision\n",
    "\n",
    "#Parameters (feel free to change these)\n",
    "#For simplicity in this example, we will only consider (near) circular orbits \n",
    "#  (I've found that having exactly zero for e and omega can \"break\" the fitters for some reason)\n",
    "#  but feel free to explore non-zero eccentricites on your own after completing this workshop!\n",
    "T0 = drange/5.  #days (generally in JD) at periastron\n",
    "P = 0.7*drange   #period in days\n",
    "e = 0.001   #eccentricity\n",
    "V0 = 25.    #center-of-mass velocity km/s\n",
    "K = 10.     #amplitude of signal km/s (NOTE: this encodes a lot of hidden information, e.g. masses, inclination)\n",
    "omega = 0.001 #longitude at peri (this is not well defined for e = 0, because there is no peri)\n",
    "params = [T0, P, V0, K, e, omega]\n",
    "params_name = ['T0', 'P', r'$\\gamma$', 'K', 'e', r'$\\omega$']\n",
    "\n",
    "#Obtaining the true radial velocities based on the parameters above\n",
    "RV_true = helio_rv(JD, params) #km/s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Now add in noise to account for the measurement precision  (i.e., magnitude of the typical error)\n",
    "noise_lvl = 0.5 #km/s, single measurement precision, (i.e. the error on each measurement)\n",
    "\n",
    "#in order to add this noise to the true observations, you need to draw from a Gaussian with sigma = noise_lvl\n",
    "RV_obs = RV_true + ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#calculate the residuals from the fit:\n",
    "#create a list that contains the difference between the true model (RV_true) and the observed data (RV_obs)\n",
    "OC = ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Plot the true curve (solid line) versus the generated observations (symbols) \n",
    "#The third plot show the residuals (i.e., the difference between true curve and observations) versus the phase\n",
    "\n",
    "#to plot the curves\n",
    "Npts = 5000\n",
    "JD_x = [x/float(Npts)*(max(JD) - min(JD)) + min(JD) for x in range(Npts)]\n",
    "\n",
    "f1 = pyplot.figure(figsize=(7,7))\n",
    "\n",
    "#Plot #1\n",
    "#RV vs. JD\n",
    "ax1 = pyplot.subplot(211)\n",
    "#first, plot the curve using the helio_rv function\n",
    "ax1.plot(JD_x, helio_rv(JD_x, params),'g') #true curve\n",
    "#now plot the observed RVs\n",
    "ax1.errorbar(---, ---, yerr = ---, fmt='.',c='g') #observations\n",
    "#label the plot\n",
    "ax1.set_xlabel('JD')\n",
    "ax1.set_ylabel('RV')\n",
    "ax1.set_ylim([min(RV_obs),max(RV_obs)])\n",
    "\n",
    "#Plot #2\n",
    "#O-C vs. JD\n",
    "ax2 = pyplot.subplot(212)\n",
    "#plot the O-C residuals for the observed RVs and the helio_rv result\n",
    "ax2.errorbar(---, ---, yerr = ---, fmt='.',c='g')\n",
    "#plot a dashed line through O-C = 0 for reference\n",
    "---\n",
    "#label the plot\n",
    "ax2.set_xlabel('JD')\n",
    "ax2.set_ylabel('(O-C)')\n",
    "ax2.set_ylim([-5.*noise_lvl,5.*noise_lvl])\n",
    "\n",
    "pyplot.draw()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Determine the Best Fit using the 'Frequentist' Approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part A: \"Frequentists\" generally <a href=\"https://en.wikipedia.org/wiki/Reduced_chi-squared_statistic\">minimize the chi^2</a> (or sometimes the residuals) to fit a function to data. Let's do that!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Define the chi^2 and reduced chi^2, which are useful indicators of your goodness-of-fit\n",
    "def chi2(ps, args):\n",
    "#we will send this function four values\n",
    "#in our case:\n",
    "#  x contains the JD values \n",
    "#  obs contains the RV observations \n",
    "#  sigma contains the uncertainties on the Rv observations\n",
    "#  func contains the function that calculates the RV curve (helio_RV)\n",
    "#But note that this is a general function that you could use in any case that you calculate chi^2\n",
    "    x,obs,sigma,func = args\n",
    "#write this function to return the result of the equation on that wikipidea page \n",
    "#  linked above shown in the section \"Calculating the test-statistic\"\n",
    "    ---\n",
    "    return ---\n",
    "\n",
    "def chi2_red(ps, args):\n",
    "    x,obs,sigma,func = args\n",
    "#now we want the reduced chi^2, which is chi2/dof, \n",
    "# dof is the number of degrees of freedom in your fit = number of observations - number of parameters\n",
    "    ---\n",
    "    return ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#compare the reduced chi^2 to our observations (with Gaussian errors).  \n",
    "#This should be about 1. \n",
    "args = (JD, RV_obs, noise_lvl, helio_rv)\n",
    "#I'm just defining some nice looking output line that we can use later on, and you'll fill in values below\n",
    "#-- you'll want to send this print statement tuples\n",
    "print '%-10s [%10s %10s %10s %10s %10s %10s] %10s' % ((\"input\",)+tuple(params_name)+(\"chi^2\",))\n",
    "print '%-10s [%10f %10f %10f %10f %10f %10f] %10f' % ((\"true\",)+tuple(params)+(chi2_red(params,args),))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In order to fit to these data by finding the minimum chi^2 value, we will need an initial guess at the parameters...<br>\n",
    "\n",
    "recall params [T0,P,VO,K,e,omega]:\n",
    "* T0 = days (generally in JD) at periastron\n",
    "* P = period in days\n",
    "* V0 = center-of-mass velocity km/s\n",
    "* K = amplitude of signal km/s \n",
    "* e = eccentricity\n",
    "* omega = longitude at peri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#let's make a semi-informed, initial guess\n",
    "guess0 = [(max(JD)-min(JD))/2., \n",
    "          (max(JD)-min(JD))/2.,\n",
    "          numpy.mean(RV_obs),\n",
    "          (max(RV_obs)-min(RV_obs))/2.,\n",
    "          0.0,\n",
    "          0.0]\n",
    "\n",
    "#check the reduced chi^2 to our guess\n",
    "print '%-10s [%10s %10s %10s %10s %10s %10s] %10s' % ((\"input\",)+tuple(params_name)+(\"chi^2\",))\n",
    "print '%-10s [%10f %10f %10f %10f %10f %10f] %10f' % ((\"true\",)+tuple(params)+(chi2_red(params,args),))\n",
    "print '%-10s [%10f %10f %10f %10f %10f %10f] %10f' % ((\"guess\",)+tuple(---)+(---,))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See how the reduced chi^2 for our guess is >> 1? This is bad!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To do: Input different parameter values in guess0 in the cell above (by changing the numbers) and rerun the cell to see how the chi^2 changes.\n",
    "\n",
    "Question:<br> \n",
    "Are you able to get a better chi^2 than we got above with our initial guess?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## In the steps below we'll try to automate minimizing the chi^2 starting from our initial-guess parameters.\n",
    "\n",
    "We will use scipy's <a href=\"http://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.leastsq.html\"> leastsq</a> function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#let's try to minimize the chi^2 starting from our initial guess\n",
    "\n",
    "#we will use leastsq from scipy for this, which performs a least squares fit (e.g., minimizing the chi^2)\n",
    "\n",
    "#leastsq requires chi^2 as a list so I'll define it here \n",
    "def chi2list(ps, args):\n",
    "    x,obs,sigma,func = args\n",
    "    OC = (obs - func(x,ps))\n",
    "    return [o**2./sigma**2. for o in OC]\n",
    " \n",
    "#here's how you can run leastsq. See the link above for more information on leastsq. \n",
    "fres = leastsq(chi2list, guess0, args = (args,), full_output = 1)\n",
    "\n",
    "# pleastsq0 = Initial parameter estimation based on trying to minimize chi^2\n",
    "pleastsq0 = fres[0]\n",
    "print '%-10s [%10s %10s %10s %10s %10s %10s] %10s' % ((\"input\",)+tuple(params_name)+(\"chi^2\",))\n",
    "print '%-10s [%10f %10f %10f %10f %10f %10f] %10f' % ((\"true\",)+tuple(params)+(chi2_red(params,args),))\n",
    "print '%-10s [%10f %10f %10f %10f %10f %10f] %10f' % ((\"guess\",)+tuple(---)+(---,))\n",
    "print '%-10s [%10f %10f %10f %10f %10f %10f] %10f' % ((\"fit0\",)+tuple(---)+(---,))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The chi^2 for the fit is better than our guess, but may still pretty bad, depending on how good your guess was (remember, we want a reduced chi^2 near 1). \n",
    "\n",
    "Below we plot the result so you can see by eye that the fit still isn't very good. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Add this minimized chi^2 fit to the figures (fit shown in cyan)\n",
    "\n",
    "f1 = pyplot.figure(figsize=(7,7))\n",
    "\n",
    "#Plot #1\n",
    "#RV vs. JD\n",
    "ax1 = pyplot.subplot(211)\n",
    "---\n",
    "\n",
    "#Overplot the minimized chi^2 fit\n",
    "ax1.plot(JD_x, helio_rv(---, ---), 'c') \n",
    "\n",
    "#Plot #2\n",
    "#O-C vs. JD\n",
    "ax2 = pyplot.subplot(212)\n",
    "---\n",
    "\n",
    "#Overplot the minimized chi^2 fit\n",
    "# Subtract the true radial velocities from the leastsq fit radial velocities\n",
    "OC_fit1 = ---\n",
    "ax2.errorbar(---, OC_fit1, yerr = noise_lvl, fmt='.',c='c')\n",
    "\n",
    "pyplot.draw()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Possibly still pretty bad, (at least it was for me!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part B: Try a Range of Initial Guesses to Try and Get a Better Fit\n",
    "\n",
    "Rather than just picking one guess, we can have python select any number of random initial guesses for us and run them through the leastsq fitter to see if any have a reduced chi^2 ~ 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here 'walkers' = initial guesses.  (We'll explain this terminology.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Set the number of walkers to use \n",
    "nwalkers = 20\n",
    "#note: emcee (below) requires an even number of walkers (initial guesses), \n",
    "#and the number of walkers must be at least twice the number of parameters that we're fitting (which is 6)\n",
    "\n",
    "#To help guide the fit, let's add some bounds for the parameters \n",
    "#NOTE: the leastsq function in scipy does not allow bounds on parameters (though other fitters do)\n",
    "#      we will simply use these bounds to define our walkers\n",
    "bnds0 = ((min(JD), max(JD)), \n",
    "        (1., max(JD)-min(JD)), \n",
    "        (min(RV_obs), max(RV_obs)), \n",
    "        (noise_lvl, 5.*(max(RV_obs)-min(RV_obs))), \n",
    "        (0., 0.01), \n",
    "        (0., 0.01))\n",
    "\n",
    "\n",
    "# we want a list of lists here such that we have initPos = [ [params1], [params2], ... ]\n",
    "# and params# contains all the parameters from our RV curve = T0, P, gamma, K, e, omega'\n",
    "# each value in params# should be drawn from a random uniform distribution between the relevant bnds0 defined above\n",
    "#\n",
    "# Hint: I found it easier to first select the parameters in an array where a given \"row\" contains \n",
    "# nwalkers random draws for the given param value, and to then transpose this matrix so that I can get \n",
    "# the list of guesses that I said we need. \n",
    "\n",
    "initPos = ---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Now let's step through these guesses and do the chi^2 minimization on each\n",
    "#Note: some don't finish successfully (see last column)\n",
    "\n",
    "#lets define some dictionary to hold the results from these fits\n",
    "fits = {\"chi2\":[], \"params\":[], \"pcov\":[]}\n",
    "\n",
    "#just for reference\n",
    "print '%-10s [%10s %10s %10s %10s %10s %10s] %10s' % ((\"input\",)+tuple(params_name)+(\"chi^2\",))\n",
    "print '%-10s [%10f %10f %10f %10f %10f %10f] %10f\\n' % ((\"true\",)+tuple(params)+(chi2_red(params,args),))\n",
    "\n",
    "for p in initPos:\n",
    "#use leastsq to get the best fit given this guess, p\n",
    "    fres = ---\n",
    "    \n",
    "    print '%-10s [%10f %10f %10f %10f %10f %10f] %10f' % ((\"guess\"+str(i+1),)+tuple(p)+(chi2_red(p,args),))\n",
    "    print '%-10s [%10f %10f %10f %10f %10f %10f] %10f\\n' % ((\"fit\"+str(i+1),)+tuple(---)+(---,))\n",
    "\n",
    "    fits[\"chi2\"].append(chi2_red(fres[0],args))\n",
    "    fits[\"params\"].append(fres[0])\n",
    "    fits[\"pcov\"].append(fres[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To do: Look at the output above showing the parameter values and the chi^2 values. Can you find the smallest chi^2 value in the list? \n",
    "\n",
    "In the step below, you'll automate finding the smallest chi^2 value from this outputted list. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Identify the best-fit from this range of guesses\n",
    "\n",
    "# numpy.nanargmin finds the minimum value in an array (which isn't 'nan' or 'inf')\n",
    "pbest = numpy.nanargmin(fits[\"chi2\"])\n",
    "\n",
    "# pleastsq gives the parameters corresponding to the minimum chi^2 value (i.e., pbest)\n",
    "pleastsq = ---\n",
    "#print that out\n",
    "print '%-24s [%10f %10f %10f %10f %10f %10f] %10f' % ((\"second fit attempt\",)+tuple(---)+(---,))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### If you don't get a result with chi^2 ~ 1, then rerun the previous 3 cells until a random guess gets close enough to give you a good fit.\n",
    "\n",
    "Below we plot the result so you can see by eye that the fit still isn't very good. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Plot this best-fit\n",
    "\n",
    "#Purple: best-fit based on larger sample of random guesses\n",
    "#Cyan: first attempt at the least-squares fit\n",
    "#Green = true curve\n",
    "\n",
    "f1 = pyplot.figure(figsize=(7,7))\n",
    "\n",
    "#Plot #1\n",
    "#RV vs. JD\n",
    "ax1 = pyplot.subplot(211)\n",
    "---\n",
    "\n",
    "#least-squares fit from larger sample of random guesses\n",
    "---\n",
    "\n",
    "#first least-squares fit\n",
    "---\n",
    "\n",
    "#Plot #2\n",
    "#O-C vs. JD\n",
    "ax2 = pyplot.subplot(212)\n",
    "---\n",
    "\n",
    "#first least-squares fit\n",
    "---\n",
    "\n",
    "#least-squares fit from larger sample of random guesses\n",
    "OC_fit2 = ---\n",
    "ax2.errorbar(---)\n",
    "\n",
    "pyplot.draw()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now the chi^2 should be much closer to 1, and the plots should show the fit is quite good as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now, what are the errors on these fit parameters?\n",
    "\n",
    "Note: occasionally I've found that leastsq does not return a covariance matrix, and in that case the code below will fail.  If that happens, rerun the above cells to find a new solution that has a covariance matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#get the errors from the covariance matrix, \n",
    "#taken from http://stackoverflow.com/questions/14581358/getting-standard-errors-on-fitted-parameters-using-the-optimize-leastsq-method-i\n",
    "pfit = fits[\"params\"][pbest]\n",
    "pcov = fits[\"pcov\"][pbest]\n",
    "pcov = pcov * chi2_red(pfit, args)\n",
    "error = [] \n",
    "for i in range(len(params)):\n",
    "    try:\n",
    "        error.append( numpy.absolute(pcov[i][i])**0.5)\n",
    "    except:\n",
    "        error.append( 0.00 )\n",
    "\n",
    "for i,p in enumerate(pfit):\n",
    "    name = params_name[i]\n",
    "    print '%-10s : %10f  +/- %10f ' % ((params_name[i],)+(p, error[i],))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions:\n",
    "* How well did this match your input values?     \n",
    "\n",
    "* In particular, did you get the right T0?  If not, why, and is this a problem?   \n",
    "\n",
    "* Did you get the right omega?  Why might this be hard to fit for the system we chose?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note: often the errors derived in this manner are either underestimated or otherwise unrealistic because the uncertainties on the observed data are poorly known (for instance see the discussion <a href=\"http://dan.iel.fm/emcee/current/user/line/\"> here</a>)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Fit a Model to the Data using the Bayesian Method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another way to fit a model to data is through the <a href=\"http://en.wikipedia.org/wiki/Bayes%27_theorem\">Bayesian</a> method. Instead of finding a \"best-fit\" solution, \"Bayesians\" want to find the distribution of solutions, called the \"posterior\". \n",
    "\n",
    "Bayes' theorem is stated mathematically as the following equation:\n",
    "\n",
    "$P(A|B) = \\frac{P(B | A)\\, P(A)}{P(B)}$,\n",
    "where A and B are events.\n",
    "\n",
    "* $P(A)$ and $P(B)$ are the probabilities of A and B without regard to each other.\n",
    "* $P(A | B)$, a conditional probability, is the probability of A given that B is true.\n",
    "* $P(B | A)$, is the probability of B given that A is true.\n",
    "\n",
    "In practice, $B$ is the model (here, the helio_rv function), and $A$ is the data (the RV observations).  We want to know the $P(A|B)$, or, in other words, how well the model describes the data.  We generally don't know $P(B)$, the instrinsic probability that our model is \"correct\".  We call $P(A)$ the \"prior\", and this defines our understanding of the instrinsic probability that a given parameter in our model has a certain value (for instance, maybe this parameter, say the orbital period, is believed to be drawn from a Guassian distribution).  $P(B|A)$ is the \"likelihood\", and can be thought of as a goodness of fit value for the model given the data.\n",
    "\n",
    "Python has a very useful tool, called 'emcee' that will run a <a href=\"http://en.wikipedia.org/wiki/Markov_chain_Monte_Carlo\">Markov Chain Monte Carlo (MCMC)</a> sampling, and return the posterior to us. Let's set this up!\n",
    "\n",
    "For additional information on 'emcee' check out <a href=\"http://dan.iel.fm/emcee/current/user/line/\">this tutorial</a>. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#For Guassian noise you can write the likelihood as exp(-chi^2/2)\n",
    "#emcee uses ln(likelihood) so...\n",
    "def lnlike(ps, args):\n",
    "    return -0.5*chi2(ps,args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: We can set bounds on the parameter values (like above) to create flat priors, if we set a prior value of 1 within the range and 0 outside of it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#To help guide the fit, let's add some bounds for the parameters \n",
    "bnds = ((min(JD), max(JD)), \n",
    "        (1., max(JD)-min(JD)), \n",
    "        (min(RV_obs), max(RV_obs)), \n",
    "        (noise_lvl, 5.*(max(RV_obs)-min(RV_obs))), \n",
    "        (0., 0.999), \n",
    "        (0., 360.))\n",
    "\n",
    "def lnprior(ps):\n",
    "    lp = 0. #ln(1)\n",
    "    for i,pp in enumerate(ps):\n",
    "        if (pp < bnds[i][0] or pp > bnds[i][1]):\n",
    "            return -numpy.inf #ln(0) if outside the bounds\n",
    "\n",
    "    return lp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#now construct the probability as priors*likelihood (but again in the log)\n",
    "def lnprob(ps, args):\n",
    "    lp = lnprior(ps)\n",
    "    if not numpy.isfinite(lp):\n",
    "        return -numpy.inf\n",
    "    return lp + lnlike(ps, args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a href=\"http://en.wikipedia.org/wiki/Markov_chain_Monte_Carlo\">Markov Chain Monte Carlo (MCMC)</a> is a method for efficiently searching through a large parameter space (like we have here).  \n",
    "\n",
    "It starts a number of \"walkers\" at their initial guesses, then walks them towards the best fit value(s) based on \"jump\" criteria (the art in the MCMC method) that allow the walker to move in this direction.   Eventually the walkers should reach this best fit \"peak\" in the posterior distribution, and spend most of their time mapping out this region.  When we get the \"sampler\" back, we will see where the walkers have been.  The \"chain\" is the path that a given walker takes through the posterior distribution.  With enough walkers and enough samples, the walkers will map out the posterior distribution for us. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#initialize the mcmc sampler\n",
    "ndim = len(params)\n",
    "sampler = emcee.EnsembleSampler(nwalkers, ndim, lnprob, args = (args,), threads = nthreads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#draw initial walkers from the covariance matrix given by the least squares fit\n",
    "#we don't have to do it this way (for instance we could simply take the random walkers we had before),\n",
    "#but this will produce a good fit more easily -- and we've already done the work above, so let's use that result.\n",
    "def getwalkers(best, cov, walkers, nwalkers, n=10):\n",
    "    pInits = numpy.random.multivariate_normal(best, cov*n, nwalkers)\n",
    "    for j,p in enumerate(pInits):\n",
    "        for i,a in enumerate(p):\n",
    "            if (a > bnds[i][1] or a < bnds[i][0]):\n",
    "                pInits[j][i] = walkers[j][i]\n",
    "    return pInits\n",
    "\n",
    "walkers = getwalkers(pfit, pcov, initPos, nwalkers)\n",
    "f4 = corner.corner(walkers, labels = params_name, truths = params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Run our MCMC :\n",
    "#(this will take some time if you have a large N. I'd recommend starting with a smaller N first to test things.)\n",
    "sampler.reset()\n",
    "\n",
    "#number of iterations to take\n",
    "N = 1000\n",
    "\n",
    "#Only record every Nthin sample\n",
    "Nthin = 10\n",
    "\n",
    "\n",
    "#Run the MCMC (using the initial random guesses, or drawn from the leastsq covariance matrix)\n",
    "#sampler.run_mcmc(initPos, N, thin = Nthin)\n",
    "sampler.run_mcmc(walkers, N, thin = Nthin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# We now have access to our posterior sample through the sampler object:  \n",
    "print \"The shape of the sampler.chain is (Nwalker , Nsamples, Nparams): \", sampler.chain.shape\n",
    "print \"The shape of the chain.flatchain is (Nwalker x Nsamples, Nparams): \", sampler.flatchain.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The \"burn in\"\n",
    "\n",
    "In general, within MCMC, the walkers need a little bit of time to determine their direction of travel, i.e., to start climbing up toward the best-fit value(s).  We call this initial bit the \"burn-in\", and we generally discard that because it is not informative about the true posterior distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#consider the first nburn as a \"burn-in\" and create the posterior distribution\n",
    "nburn = 20\n",
    "samples = sampler.chain[:, nburn:, :].reshape((-1, ndim))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the chains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pyplot.figure(figsize=(10,20))\n",
    "\n",
    "subn = len(params)*100+11\n",
    "\n",
    "#each color shows a different walker (though some colors are probably repeated)\n",
    "#solid black horizontal lines show the true values\n",
    "#to the left of the dashed vertical black lines is the (discarded) burn in\n",
    "for walker in range(sampler.chain.shape[0]):\n",
    "    for i,p in enumerate(params):\n",
    "        ax = pyplot.subplot(subn+i)\n",
    "        ax.plot(sampler.chain[walker,:,i])\n",
    "        ax.plot([0,len(sampler.chain[walker,:,i])],[p,p],'k', linewidth=2)\n",
    "        ax.plot([nburn,nburn],[min(sampler.chain[walker,:,i]),max(sampler.chain[walker,:,i])],'k--', linewidth=2)\n",
    "        ax.set_ylabel(params_name[i])\n",
    "\n",
    "pyplot.draw()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the posterior distribution resulting from these chains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# We can also plot our posterior distributions on a sweet corner plot.\n",
    "f4 = corner.corner(---)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the distribution of chi^2 values from the fit as a check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Determine the distribution of reduced chi^2 values for the MCMC run\n",
    "chi2a = [--- for --- in samples]\n",
    "\n",
    "#Plot the chi^2 distribution of MCMC fits\n",
    "pyplot.figure(figsize=(7,5))\n",
    "\n",
    "#plot a histogram of the reduced chi^2 values\n",
    "---\n",
    "\n",
    "pyplot.xlabel(r'log($\\chi^2$)')\n",
    "pyplot.ylabel('N')\n",
    "\n",
    "pyplot.draw()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the emcee fit on top of our other fits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Plot this best-fit(s)\n",
    "\n",
    "#Purple: best-fit based on larger sample of random guesses\n",
    "#Cyan: first attempt at the least-squares fit\n",
    "#Green:  true curve\n",
    "#Orange: MCMC fit\n",
    "\n",
    "---\n",
    "\n",
    "#MCMC fit\n",
    "#for this one we want to select some number of random samples from the posterior to plot\n",
    "Nvals = ---\n",
    "for i in range(Nvals):\n",
    "#choose some random list position\n",
    "    pos = ---\n",
    "    pMCMC = samples[pos]\n",
    "\n",
    "    ax1.plot(---, 'orange', alpha = 0.2) \n",
    "    OC_MCMC = ---\n",
    "    ax2.errorbar(---, c='orange', alpha = 0.2)\n",
    "\n",
    "pyplot.draw()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A benefit of MCMC is that you can be clear about your uncertainties."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#What do we quote as our results when we publish and share with our colleagues?\n",
    "\n",
    "#We could quote the following. This will give the 50% percentile and \n",
    "#then the +/- \"1 sigma\" error bars\n",
    "pr_lo = 16.\n",
    "pr_mi = 50.\n",
    "pr_hi = 84.\n",
    "\n",
    "pout = map(lambda v: (v[1], v[2]-v[1], v[1]-v[0]), zip(*numpy.percentile(samples, [pr_lo, pr_mi, pr_hi], axis=0)))\n",
    "for i,p in enumerate(pout):\n",
    "    print '%-10s : %10f  +%10f  -%10f' % ((params_name[i],)+tuple(p))  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions: \n",
    "* How do these uncertainties compare to those that you found for the leastsq fit?   \n",
    "    \n",
    "* Why do we have different + and - errors here, and only one +/- value above?\n",
    "\n",
    "* Which uncertainties do you trust more?  Why?\n",
    "\n",
    "#### Note: that you have more information in the Bayesian fit than just the uncertainties that we printed here. You have the full posterior distributions for all parameters.  Therefore you also have a much more detailed understanding of how parameters relate to eachother (e.g., if two or more are correlated)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: To do\n",
    "\n",
    "### Go back to the start of Step 5 above. How well do you understand each step? What happens if you change different values? \n",
    "\n",
    "For example:\n",
    "* Change the number of iterations\n",
    "* Change the sampling rate (Nthin)\n",
    "* Change nburn\n",
    "* etc.\n",
    "\n",
    "As you make these changes, see if you can continue to deepen your understanding of how emcee works and what the results mean. Don't hesitate to ask questions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Congratulations! You've completed Part 6!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extension Activity: Download observations and perform your own fit to a real-life binary star!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Download some data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One nice set of observed data is <a href=\"http://cdsarc.u-strasbg.fr/viz-bin/Cat?cat=J%2FAJ%2F137%2F3743&target=brief&\">Table 1 from Geller et al. (2009)</a> on Vizier. \n",
    "\n",
    "Note: You may find it convenient to modify the table to only include SB1 binaries:<br>\n",
    "cat Geller2009_table1.dat | awk '{if (NF == 7) print $0}' >! Geller2009_table1.dat.mod\n",
    "\n",
    "Also, if you want to you ascii.read, you'll need to add a header\n",
    "\n",
    "We've done all this for you, and placed <a href=\"data/Geller2009_table1.dat.mod\">Geller2009_table1.dat.mod</a> in your data folder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Step 2: Create cells below (following what we did above) to fit an orbit to these data\n",
    "\n",
    "Note: one more elegant way to code in python is to create a <a href=\"https://docs.python.org/2/tutorial/classes.html\"> Python class</a>.  For instance, you could create a class that has this entire fitter in it.  And then you can use that fitter for any set of data.  I encourage you to try this!  If you are interested and have questions, please ask.\n",
    "\n",
    "#### You can check how well your results match the fits from our \"frequentist\" method by looking up these binaries in <a href=\"http://adsabs.harvard.edu/abs/2009AJ....137.3743G\">our paper</a>!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Further Extension Activity: Find another set of observed data to run MCMC on :)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
